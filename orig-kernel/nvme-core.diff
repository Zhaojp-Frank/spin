--- ./drivers/block/nvme-core.c	2018-08-08 23:45:55.799298596 -0400
+++ ../../spin/spinnvme/nvme-core.c	2018-08-09 00:01:38.873353807 -0400
@@ -40,6 +40,9 @@
 #include <linux/types.h>
 #include <scsi/sg.h>
 #include <asm-generic/io-64-nonatomic-lo-hi.h>
+#define SHAIDEBUG
+
+#define MYFUNC
 
 #define NVME_Q_DEPTH		1024
 #define NVME_AQ_DEPTH		64
@@ -90,6 +93,19 @@ struct async_cmd_info {
 	void *ctx;
 };
 
+#define DIO_PAGES	64
+
+//Macro to check arch_1 flag
+TESTPAGEFLAG(f,arch_1)
+//TEST
+#ifdef MYFUNC
+extern u64 get_dma(u64* length);
+extern void add_device(int major);
+extern void remove_device(int major);
+#endif
+//END
+
+
 /*
  * An NVM Express queue.  Each device has at least two (one for admin
  * commands and one for I/O commands).
@@ -117,6 +133,44 @@ struct nvme_queue {
 	struct blk_mq_hw_ctx *hctx;
 };
 
+
+// frank .. new 
+struct dio {
+	int flags;			/* doesn't change */
+	int rw;
+	struct inode *inode;
+	loff_t i_size;			/* i_size when submitted */
+	dio_iodone_t *end_io;		/* IO completion function */
+
+	void *private;			/* copy from map_bh.b_private */
+
+	/* BIO completion state */
+	spinlock_t bio_lock;		/* protects BIO fields below */
+	int page_errors;		/* errno from get_user_pages() */
+	int is_async;			/* is IO async ? */
+	bool defer_completion;		/* defer AIO completion to workqueue? */
+	int io_error;			/* IO error in completion path */
+	unsigned long refcount;		/* direct_io_worker() and bios */
+	struct bio *bio_list;		/* singly linked via bi_private */
+	struct task_struct *waiter;	/* waiting task (NULL if none) */
+
+	/* AIO related stuff */
+	struct kiocb *iocb;		/* kiocb */
+	ssize_t result;                 /* IO result */
+
+	/*
+	 * pages[] (and any fields placed after it) are not zeroed out at
+	 * allocation time.  Don't add new fields after pages[] unless you
+	 * wish that they not be zeroed.
+	 */
+	union {
+		struct page *pages[DIO_PAGES];	/* page buffer */
+		struct work_struct complete_work;/* deferred AIO completion */
+	};
+} ____cacheline_aligned_in_smp;
+
+
+
 /*
  * Check we didin't inadvertently grow the command struct
  */
@@ -259,6 +314,8 @@ static void *cancel_cmd_info(struct nvme
 static void async_req_completion(struct nvme_queue *nvmeq, void *ctx,
 						struct nvme_completion *cqe)
 {
+	struct request *req = ctx;
+
 	u32 result = le32_to_cpup(&cqe->result);
 	u16 status = le16_to_cpup(&cqe->status) >> 1;
 
@@ -267,6 +324,8 @@ static void async_req_completion(struct
 	if (status == NVME_SC_SUCCESS)
 		dev_warn(nvmeq->q_dmadev,
 			"async event result %08x\n", result);
+
+	blk_mq_free_hctx_request(nvmeq->hctx, req);
 }
 
 static void abort_completion(struct nvme_queue *nvmeq, void *ctx,
@@ -368,7 +427,7 @@ static int nvme_npages(unsigned size, st
 	return DIV_ROUND_UP(8 * nprps, dev->page_size - 8);
 }
 
-static struct nvme_iod *
+	static struct nvme_iod *
 nvme_alloc_iod(unsigned nseg, unsigned nbytes, struct nvme_dev *dev, gfp_t gfp)
 {
 	struct nvme_iod *iod = kmalloc(sizeof(struct nvme_iod) +
@@ -455,8 +514,8 @@ static void req_completion(struct nvme_q
 }
 
 /* length is in bytes.  gfp flags indicates whether we may sleep. */
-int nvme_setup_prps(struct nvme_dev *dev, struct nvme_iod *iod, int total_len,
-								gfp_t gfp)
+int nvme_setup_prps_e(struct nvme_dev *dev, struct nvme_iod *iod, int total_len,
+		gfp_t gfp, bool from_blk)
 {
 	struct dma_pool *pool;
 	int length = total_len;
@@ -469,11 +528,34 @@ int nvme_setup_prps(struct nvme_dev *dev
 	__le64 **list = iod_list(iod);
 	dma_addr_t prp_dma;
 	int nprps, i;
+	u64* helper;
+	//Check whether the page is marked. If so, update the dma_addr.
+	if(from_blk && Pagef(pfn_to_page(PFN_DOWN((dma_addr)))))
+	{
+		//TEST
+		//END TEST
+		//Read GPU phy addressfrom the begginning of the page and add the offset
+		helper = ((u64 *)(phys_to_virt((dma_addr & (u64)(~0xFFF)))));
+#ifdef MYFUNC
+
+//printk("Thread %d\n",current->pid);
+//		dma_addr = cpu_to_le64(get_dma(helper[1], helper[0]) + offset);
+		dma_addr = cpu_to_le64(get_dma(helper) + offset);
+#else
+
+		dma_addr = cpu_to_le64(*(helper) + offset);
+#endif
+/*OLD
+		//dma_addr = cpu_to_le64(*((u64 *)(phys_to_virt((dma_addr & (u64)(~0xFFF))))) + offset);
+*/
+		//printk("%p %p\n",dma_addr,get_dma(cpu_to_le64(*((u64 *)((phys_to_virt((dma_addr & (u64)(~0xFFF))))+8)))));
+		sg->dma_address = dma_addr;
+	}
 
 	length -= (page_size - offset);
-	if (length <= 0)
+	if (length <= 0){
 		return total_len;
-
+	}
 	dma_len -= (page_size - offset);
 	if (dma_len) {
 		dma_addr += (page_size - offset);
@@ -481,6 +563,25 @@ int nvme_setup_prps(struct nvme_dev *dev
 		sg = sg_next(sg);
 		dma_addr = sg_dma_address(sg);
 		dma_len = sg_dma_len(sg);
+
+		//translate if needed
+		if(from_blk && Pagef(pfn_to_page(PFN_DOWN((dma_addr))))){
+			helper = ((u64 *)(phys_to_virt((dma_addr & (u64)(~0xFFF)))));
+#ifdef MYFUNC
+
+//printk("Thread %d\n",current->pid);
+			//dma_addr = cpu_to_le64(get_dma(helper[1],helper[0]) + (dma_addr & (page_size - 1)));
+			dma_addr = cpu_to_le64(get_dma(helper) + (dma_addr & (page_size - 1)));
+#else
+
+			dma_addr = cpu_to_le64(*(helper) + (dma_addr & (page_size - 1)));
+#endif
+/*OLD 
+			dma_addr = cpu_to_le64(*((u64 *)(phys_to_virt((dma_addr & (u64)(~0xFFF))))) + (dma_addr & (page_size - 1)));
+*/
+			sg->dma_address = dma_addr;
+		}
+
 	}
 
 	if (length <= page_size) {
@@ -508,10 +611,12 @@ int nvme_setup_prps(struct nvme_dev *dev
 	i = 0;
 	for (;;) {
 		if (i == page_size >> 3) {
+
 			__le64 *old_prp_list = prp_list;
 			prp_list = dma_pool_alloc(pool, gfp, &prp_dma);
-			if (!prp_list)
+			if (!prp_list){
 				return total_len - length;
+			}
 			list[iod->npages++] = prp_list;
 			prp_list[0] = old_prp_list[i - 1];
 			old_prp_list[i - 1] = cpu_to_le64(prp_dma);
@@ -523,14 +630,43 @@ int nvme_setup_prps(struct nvme_dev *dev
 		length -= page_size;
 		if (length <= 0)
 			break;
-		if (dma_len > 0)
+		if (dma_len > 0){
 			continue;
+		}
 		BUG_ON(dma_len < 0);
 		sg = sg_next(sg);
 		dma_addr = sg_dma_address(sg);
 		dma_len = sg_dma_len(sg);
+
+		//translate if needed
+		if(from_blk && Pagef(pfn_to_page(PFN_DOWN((dma_addr))))){
+			helper =((u64 *)(phys_to_virt((dma_addr)))); 
+
+#ifdef MYFUNC
+
+
+//printk("Thread %d\n",current->pid);
+		  //dma_addr = cpu_to_le64(get_dma(helper[1],helper[0]));
+		  dma_addr = cpu_to_le64(get_dma(helper));
+
+
+
+
+#else
+
+                dma_addr = cpu_to_le64(*(helper));
+#endif
+
+			/*OLD
+			dma_addr = cpu_to_le64(*((u64 *)(phys_to_virt((dma_addr)))));
+*/
+			sg->dma_address = dma_addr;
+		} 
+
 	}
 
+
+
 	return total_len;
 }
 
@@ -539,6 +676,9 @@ int nvme_setup_prps(struct nvme_dev *dev
  * worth having a special pool for these or additional cases to handle freeing
  * the iod.
  */
+int nvme_setup_prps(struct nvme_dev *dev, struct nvme_iod *iod, int total_len,gfp_t gfp){
+	return nvme_setup_prps_e(dev, iod, total_len, gfp, false);
+}
 static void nvme_submit_discard(struct nvme_queue *nvmeq, struct nvme_ns *ns,
 		struct request *req, struct nvme_iod *iod)
 {
@@ -659,7 +801,7 @@ static int nvme_queue_rq(struct blk_mq_h
 			goto retry_cmd;
 
 		if (blk_rq_bytes(req) !=
-                    nvme_setup_prps(nvmeq->dev, iod, blk_rq_bytes(req), GFP_ATOMIC)) {
+				nvme_setup_prps_e(nvmeq->dev, iod, blk_rq_bytes(req), GFP_ATOMIC, true)) {
 			dma_unmap_sg(&nvmeq->dev->pci_dev->dev, iod->sg,
 					iod->nents, dma_dir);
 			goto retry_cmd;
@@ -679,10 +823,10 @@ static int nvme_queue_rq(struct blk_mq_h
 	spin_unlock_irq(&nvmeq->q_lock);
 	return BLK_MQ_RQ_QUEUE_OK;
 
- error_cmd:
+error_cmd:
 	nvme_free_iod(nvmeq->dev, iod);
 	return BLK_MQ_RQ_QUEUE_ERROR;
- retry_cmd:
+retry_cmd:
 	nvme_free_iod(nvmeq->dev, iod);
 	return BLK_MQ_RQ_QUEUE_BUSY;
 }
@@ -831,19 +976,18 @@ static int nvme_submit_async_admin_req(s
 	struct nvme_cmd_info *cmd_info;
 	struct request *req;
 
-	req = blk_mq_alloc_request(dev->admin_q, WRITE, GFP_ATOMIC, true);
+	req = blk_mq_alloc_request(dev->admin_q, WRITE, GFP_ATOMIC, false);
 	if (IS_ERR(req))
 		return PTR_ERR(req);
 
 	req->cmd_flags |= REQ_NO_TIMEOUT;
 	cmd_info = blk_mq_rq_to_pdu(req);
-	nvme_set_info(cmd_info, NULL, async_req_completion);
+	nvme_set_info(cmd_info, req, async_req_completion);
 
 	memset(&c, 0, sizeof(c));
 	c.common.opcode = nvme_admin_async_event;
 	c.common.command_id = req->tag;
 
-	blk_mq_free_hctx_request(nvmeq->hctx, req);
 	return __nvme_submit_cmd(nvmeq, &c);
 }
 
@@ -1029,7 +1172,7 @@ static void nvme_abort_req(struct reques
 							req->tag, nvmeq->qid);
 		dev->reset_workfn = nvme_reset_failed_dev;
 		queue_work(nvme_workq, &dev->reset_work);
- out:
+out:
 		spin_unlock_irqrestore(&dev_list_lock, flags);
 		return;
 	}
@@ -1239,10 +1382,10 @@ static struct nvme_queue *nvme_alloc_que
 
 	return nvmeq;
 
- free_cqdma:
+free_cqdma:
 	dma_free_coherent(dmadev, CQ_SIZE(depth), (void *)nvmeq->cqes,
 							nvmeq->cq_dma_addr);
- free_nvmeq:
+free_nvmeq:
 	kfree(nvmeq);
 	return NULL;
 }
@@ -1293,9 +1436,9 @@ static int nvme_create_queue(struct nvme
 	nvme_init_queue(nvmeq, qid);
 	return result;
 
- release_sq:
+release_sq:
 	adapter_delete_sq(dev, qid);
- release_cq:
+release_cq:
 	adapter_delete_cq(dev, qid);
 	return result;
 }
@@ -1403,7 +1546,6 @@ static int nvme_alloc_admin_tags(struct
 		dev->admin_tagset.ops = &nvme_mq_admin_ops;
 		dev->admin_tagset.nr_hw_queues = 1;
 		dev->admin_tagset.queue_depth = NVME_AQ_DEPTH - 1;
-		dev->admin_tagset.reserved_tags = 1;
 		dev->admin_tagset.timeout = ADMIN_TIMEOUT;
 		dev->admin_tagset.numa_node = dev_to_node(&dev->pci_dev->dev);
 		dev->admin_tagset.cmd_size = sizeof(struct nvme_cmd_info);
@@ -1488,11 +1630,48 @@ static int nvme_configure_admin_queue(st
 
 	return result;
 
- free_nvmeq:
+free_nvmeq:
 	nvme_free_queues(dev, 0);
 	return result;
 }
 
+static int handle_pfn_pages(struct scatterlist *sg, unsigned long start,
+		unsigned long length)
+{
+	int i = 0;
+	struct vm_area_struct *vma = NULL;
+	unsigned long pfn;
+	struct mm_struct *mm = current->mm;
+
+
+	do {
+		vma = find_vma(mm, start);
+		if (!vma || !(vma->vm_flags & VM_PFNMAP))
+			return -EFAULT;
+
+		sg[i].page_link = 0;
+
+		if (follow_pfn(vma, start, &pfn))
+			return -EINVAL;
+
+		sg[i].dma_address = (pfn << PAGE_SHIFT) + (start & ~PAGE_MASK);
+		sg[i].length = min_t(unsigned, length, vma->vm_end - start);
+		sg[i].dma_length = sg[i].length;
+		sg[i].offset = 0;
+
+
+		length -= sg[i].dma_length;
+		start += sg[i].dma_length;
+		i++;
+
+	} while (length);
+
+
+	sg_mark_end(&sg[i - 1]);
+
+	return 0;
+}
+
 struct nvme_iod *nvme_map_user_pages(struct nvme_dev *dev, int write,
 				unsigned long addr, unsigned length)
 {
@@ -1512,18 +1691,27 @@ struct nvme_iod *nvme_map_user_pages(str
 	if (!pages)
 		return ERR_PTR(-ENOMEM);
 
-	err = get_user_pages_fast(addr, count, 1, pages);
-	if (err < count) {
-		count = err;
-		err = -EFAULT;
-		goto put_pages;
-	}
-
 	err = -ENOMEM;
 	iod = nvme_alloc_iod(count, length, dev, GFP_KERNEL);
 	if (!iod)
 		goto put_pages;
 
+	err = get_user_pages_fast(addr, count, 1, pages);
+	if (err == -EFAULT) {
+		sg_init_table(iod->sg, count);
+		if (handle_pfn_pages(iod->sg, addr, length)) {
+			err = -EFAULT;
+			goto free_iod;
+		}
+
+		kfree(pages);
+		return iod;
+	} else if (err < count) {
+		count = err;
+		err = -EFAULT;
+		goto free_iod;
+	}
+
 	sg = iod->sg;
 	sg_init_table(sg, count);
 	for (i = 0; i < count; i++) {
@@ -1544,9 +1732,9 @@ struct nvme_iod *nvme_map_user_pages(str
 	kfree(pages);
 	return iod;
 
- free_iod:
+free_iod:
 	kfree(iod);
- put_pages:
+put_pages:
 	for (i = 0; i < count; i++)
 		put_page(pages[i]);
 	kfree(pages);
@@ -1565,6 +1753,39 @@ void nvme_unmap_user_pages(struct nvme_d
 		put_page(sg_page(&iod->sg[i]));
 }
 
+struct nvme_iod *nvme_handle_physical_address(struct nvme_dev *dev, int write,
+		unsigned long addr, unsigned length)
+{
+	int err, count;
+	struct scatterlist *sg;
+	struct nvme_iod *iod;
+
+	if (addr & 3)
+		return ERR_PTR(-EINVAL);
+	if (!length || length > INT_MAX - PAGE_SIZE)
+		return ERR_PTR(-EINVAL);
+
+	count = 1;
+
+	err = -ENOMEM;
+	iod = nvme_alloc_iod(count, length, dev, GFP_KERNEL);
+	if (!iod)
+		return ERR_PTR(err);
+
+	/* initialize iod */
+	sg = iod->sg;
+	sg_init_table(sg, count);
+
+	sg[0].page_link = 0;
+
+	sg[0].dma_address = addr;
+	sg[0].length = length;
+	sg[0].dma_length = sg[0].length;
+	sg[0].offset = 0;
+
+	return iod;
+}
+
 static int nvme_submit_io(struct nvme_ns *ns, struct nvme_user_io __user *uio)
 {
 	struct nvme_dev *dev = ns->dev;
@@ -1588,7 +1809,17 @@ static int nvme_submit_io(struct nvme_ns
 	case nvme_cmd_write:
 	case nvme_cmd_read:
 	case nvme_cmd_compare:
+			/* testing - assaf */
+			if (io.flags != 0)
+			{
+				io.flags = 0;
+				iod = nvme_handle_physical_address(dev, io.opcode & 1, io.addr, length);
+			}
+			else
+			{
+				/* standard operation */
 		iod = nvme_map_user_pages(dev, io.opcode & 1, io.addr, length);
+			}
 		break;
 	default:
 		return -EINVAL;
@@ -1641,15 +1874,15 @@ static int nvme_submit_io(struct nvme_ns
 		c.rw.metadata = cpu_to_le64(meta_dma_addr);
 	}
 
-	length = nvme_setup_prps(dev, iod, length, GFP_KERNEL);
+	length = nvme_setup_prps_e(dev, iod, length, GFP_KERNEL, false);
 	c.rw.prp1 = cpu_to_le64(sg_dma_address(iod->sg));
 	c.rw.prp2 = cpu_to_le64(iod->first_dma);
 
 	if (length != (io.nblocks + 1) << ns->lba_shift)
 		status = -ENOMEM;
-	else
+	else{
 		status = nvme_submit_io_cmd(dev, ns, &c, NULL);
-
+	}
 	if (meta_len) {
 		if (status == NVME_SC_SUCCESS && !(io.opcode & 1)) {
 			int meta_offset = 0;
@@ -1668,7 +1901,7 @@ static int nvme_submit_io(struct nvme_ns
 								meta_dma_addr);
 	}
 
- unmap:
+unmap:
 	nvme_unmap_user_pages(dev, io.opcode & 1, iod);
 	nvme_free_iod(dev, iod);
 
@@ -1713,7 +1946,7 @@ static int nvme_user_cmd(struct nvme_dev
 								length);
 		if (IS_ERR(iod))
 			return PTR_ERR(iod);
-		length = nvme_setup_prps(dev, iod, length, GFP_KERNEL);
+		length = nvme_setup_prps_e(dev, iod, length, GFP_KERNEL, false);
 		c.common.prp1 = cpu_to_le64(sg_dma_address(iod->sg));
 		c.common.prp2 = cpu_to_le64(iod->first_dma);
 	}
@@ -1847,7 +2080,7 @@ static int nvme_revalidate_disk(struct g
 
 	blk_queue_logical_block_size(ns->queue, 1 << ns->lba_shift);
 	set_capacity(disk, le64_to_cpup(&id->nsze) << (ns->lba_shift - 9));
- free:
+free:
 	dma_free_coherent(&dev->pci_dev->dev, 4096, id, dma_addr);
 	return 0;
 }
@@ -1969,9 +2202,9 @@ static struct nvme_ns *nvme_alloc_ns(str
 
 	return ns;
 
- out_free_queue:
+out_free_queue:
 	blk_cleanup_queue(ns->queue);
- out_free_ns:
+out_free_ns:
 	kfree(ns);
 	return NULL;
 }
@@ -2082,7 +2315,7 @@ static int nvme_setup_io_queues(struct n
 
 	return 0;
 
- free_queues:
+free_queues:
 	nvme_free_queues(dev, 1);
 	return result;
 }
@@ -2121,6 +2354,7 @@ static int nvme_dev_add(struct nvme_dev
 	dev->oncs = le16_to_cpup(&ctrl->oncs);
 	dev->abort_limit = ctrl->acl + 1;
 	dev->vwc = ctrl->vwc;
+	dev->event_limit = min(ctrl->aerl + 1, 8);
 	memcpy(dev->serial, ctrl->sn, sizeof(ctrl->sn));
 	memcpy(dev->model, ctrl->mn, sizeof(ctrl->mn));
 	memcpy(dev->firmware_rev, ctrl->fr, sizeof(ctrl->fr));
@@ -2170,11 +2404,13 @@ static int nvme_dev_add(struct nvme_dev
 		if (ns)
 			list_add_tail(&ns->list, &dev->namespaces);
 	}
-	list_for_each_entry(ns, &dev->namespaces, list)
+	list_for_each_entry(ns, &dev->namespaces, list){
 		add_disk(ns->disk);
+		add_device(ns->disk->major);
+}
 	res = 0;
 
- out:
+out:
 	dma_free_coherent(&dev->pci_dev->dev, 8192, mem, dma_addr);
 	return res;
 }
@@ -2227,12 +2463,12 @@ static int nvme_dev_map(struct nvme_dev
 
 	return 0;
 
- unmap:
+unmap:
 	iounmap(dev->bar);
 	dev->bar = NULL;
- disable:
+disable:
 	pci_release_regions(pdev);
- disable_pci:
+disable_pci:
 	pci_disable_device(pdev);
 	return result;
 }
@@ -2397,9 +2633,9 @@ static void nvme_disable_io_queues(struc
 }
 
 /*
-* Remove the node from the device list and check
-* for whether or not we need to stop the nvme_thread.
-*/
+ * Remove the node from the device list and check
+ * for whether or not we need to stop the nvme_thread.
+ */
 static void nvme_dev_list_remove(struct nvme_dev *dev)
 {
 	struct task_struct *tmp = NULL;
@@ -2475,8 +2711,10 @@ static void nvme_dev_remove(struct nvme_
 	struct nvme_ns *ns;
 
 	list_for_each_entry(ns, &dev->namespaces, list) {
-		if (ns->disk->flags & GENHD_FL_UP)
+		if (ns->disk->flags & GENHD_FL_UP){
 			del_gendisk(ns->disk);
+			remove_device(ns->disk->major);
+	}	
 		if (!blk_queue_dying(ns->queue)) {
 			blk_mq_abort_requeue_list(ns->queue);
 			blk_cleanup_queue(ns->queue);
@@ -2668,15 +2906,14 @@ static int nvme_dev_start(struct nvme_de
 
 	nvme_set_irq_hints(dev);
 
-	dev->event_limit = 1;
 	return result;
 
- free_tags:
+free_tags:
 	nvme_dev_remove_admin(dev);
- disable:
+disable:
 	nvme_disable_queue(dev, 0);
 	nvme_dev_list_remove(dev);
- unmap:
+unmap:
 	nvme_dev_unmap(dev);
 	return result;
 }
@@ -2805,20 +3042,20 @@ static int nvme_probe(struct pci_dev *pd
 	dev->initialized = 1;
 	return 0;
 
- remove:
+remove:
 	nvme_dev_remove(dev);
 	nvme_dev_remove_admin(dev);
 	nvme_free_namespaces(dev);
- shutdown:
+shutdown:
 	nvme_dev_shutdown(dev);
- release_pools:
+release_pools:
 	nvme_free_queues(dev, 0);
 	nvme_release_prp_pools(dev);
- release:
+release:
 	nvme_release_instance(dev);
- put_pci:
+put_pci:
 	pci_dev_put(dev->pci_dev);
- free:
+free:
 	kfree(dev->queues);
 	kfree(dev->entry);
 	kfree(dev);
@@ -2939,13 +3176,17 @@ static int __init nvme_init(void)
 		nvme_major = result;
 
 	result = pci_register_driver(&nvme_driver);
-	if (result)
+	if (result){
+	printk("PCI registered nvme dev %d\n",result);
 		goto unregister_blkdev;
+}
+printk("FINAL %d\n",nvme_major);
+
 	return 0;
 
- unregister_blkdev:
+unregister_blkdev:
 	unregister_blkdev(nvme_major, "nvme");
- kill_workq:
+kill_workq:
 	destroy_workqueue(nvme_workq);
 	return result;
 }
